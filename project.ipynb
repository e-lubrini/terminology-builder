{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/elisa/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/elisa/.local/lib/python3.8/site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/elisa/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/elisa/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/elisa/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/elisa/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn_crfsuite in /home/elisa/.local/lib/python3.8/site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/elisa/.local/lib/python3.8/site-packages (from sklearn_crfsuite) (0.9.7)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sklearn_crfsuite) (1.14.0)\n",
      "Requirement already satisfied: tabulate in /home/elisa/.local/lib/python3.8/site-packages (from sklearn_crfsuite) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in /home/elisa/.local/lib/python3.8/site-packages (from sklearn_crfsuite) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp = spacy.load(\"en_core_web_sm\")):\n",
    "        \n",
    "    special_case = [{ORTH: \"<bos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<bos>\", special_case)\n",
    "\n",
    "    special_case = [{ORTH: \"<eos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<eos>\", special_case)\n",
    "\n",
    "    infixes = list([r\"'s\\b\", r\"(?<!\\d)\\.(?!\\d)\"]) +  nlp.Defaults.prefixes\n",
    "    infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "    \n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp = spacy.load(\"en_core_web_sm\")):\n",
    "    \n",
    "    special_case = [{ORTH: \"<bos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<bos>\", special_case)\n",
    "\n",
    "    special_case = [{ORTH: \"<eos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<eos>\", special_case)\n",
    "\n",
    "    infixes = list([r\"'s\\b\", r\"(?<!\\d)\\.(?!\\d)\"]) +  nlp.Defaults.prefixes\n",
    "    infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "    \n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesExtraction:\n",
    "    def __init__(self, number=20, verbose=True, save_txt=True, save_pdf=True):\n",
    "        self.number = number\n",
    "        self.verbose = verbose\n",
    "        self.save_txt = save_txt\n",
    "        self.save_pdf = save_pdf\n",
    "\n",
    "    def _get_links(self):\n",
    "        if self.verbose:\n",
    "            print(f'Getting the links for {self.number} articles...')\n",
    "        mainpage = requests.get('https://as-botanicalstudies.springeropen.com/articles')\n",
    "        mainsoup = BeautifulSoup(mainpage.text)\n",
    "        links = ['https://as-botanicalstudies.springeropen.com' + x['href'] for x in\n",
    "                 sum([x.findAll('a') for x in mainsoup.findAll('h3', class_=\"c-listing__title\")], [])]\n",
    "        return links[:self.number]\n",
    "\n",
    "    def extract(self):\n",
    "        extra = ['Availability of data and materials', 'Abbreviations', 'References', 'Acknowledgements',\n",
    "                 'Funding', 'Author information', 'Ethics declarations', 'Additional information',\n",
    "                 'Rights and permissions', 'About this article']\n",
    "        links = self._get_links()\n",
    "        pdf_links = []\n",
    "        if self.verbose:\n",
    "            print('Getting the texts...')\n",
    "        texts = dict()\n",
    "        for num, link in enumerate(links):\n",
    "            if self.verbose:\n",
    "                print(f'{num + 1}/{len(links)} links', end=\"\\r\")\n",
    "            page = requests.get(link)\n",
    "            pagecontent = BeautifulSoup(page.text)\n",
    "            name = pagecontent.findAll('h1', class_=\"c-article-title\")[0].text\n",
    "            text = \"\\n\".join(sum([list(map(lambda y: y.text, x.findAll('p'))) for x in pagecontent.findAll('section') if\n",
    "                                  x.has_attr('data-title') and x['data-title'] not in extra], []))\n",
    "            texts[name] = text\n",
    "            pdf_link = [x.findAll('a') for x in pagecontent.findAll('div', class_=\"c-pdf-download u-clear-both\")][0][0][\n",
    "                'href']\n",
    "            pdf_links.append(pdf_link)\n",
    "        if self.save_txt:\n",
    "            if self.verbose:\n",
    "                print('Saving the articles in txt...')\n",
    "            if not os.path.exists('articles'):\n",
    "                os.mkdir('articles')\n",
    "            for key, value in texts.items():\n",
    "                with open(f\"articles/{key.replace('/', '|')}.txt\", 'w') as file:\n",
    "                    file.write(value)\n",
    "        if self.save_pdf:\n",
    "            if self.verbose:\n",
    "                print('Saving the articles in pdf...')\n",
    "            if not os.path.exists('articles_pdf'):\n",
    "                os.mkdir('articles_pdf')\n",
    "            for (key, value), link in zip(texts.items(), pdf_links):\n",
    "                pdf = requests.get('https:' + link, allow_redirects=True)\n",
    "                open(f\"articles_pdf/{key.replace('/', '|')}.pdf\", 'wb').write(pdf.content)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "\n",
    "    def extract(self, texts):\n",
    "        all_terms = []\n",
    "        self._add_rules()\n",
    "        for num, text in enumerate(texts):\n",
    "            doc = self.nlp(text)\n",
    "            matches = self.matcher(doc)\n",
    "            for match_id, start, end in matches:\n",
    "                string_id = self.nlp.vocab.strings[match_id]\n",
    "                span = doc[start:end]\n",
    "                lemma = ' '.join([n.lemma_ for n in self.nlp(span.text.lower())])\n",
    "                all_terms.append(lemma)\n",
    "\n",
    "            print(f'{num + 1}/{len(texts)} texts processed', end=\"\\r\")\n",
    "        return all_terms\n",
    "\n",
    "    def _add_rules(self):\n",
    "        noun_pattern = {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        det_pattern = {\"POS\": {\"IN\": [\"DET\", \"PRON\"]}, \"OP\": \"?\"}\n",
    "        pattern = [  #[{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, noun_pattern],\n",
    "            #[{\"DEP\": \"compound\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, noun_pattern],\n",
    "            [{\"POS\": \"ADJ\", \"OP\": \"+\"}, noun_pattern],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, det_pattern, noun_pattern],\n",
    "            [det_pattern, {\"POS\": \"ADJ\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"ADJ\"}, noun_pattern],\n",
    "        ]\n",
    "        self.matcher.add(\"terms\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator:\n",
    "    def __init__(self):        \n",
    "        self.nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "    def annotate_text(self, text, all_terms):\n",
    "        doc = self.nlp(text)\n",
    "        true_tokens = [x.text_with_ws for x in doc]\n",
    "        positions = []\n",
    "        tokens = [x.lemma_.lower() for x in doc]\n",
    "        for term in all_terms:\n",
    "            term = term.split()\n",
    "            while self._contains(term, tokens):\n",
    "                pos1, pos2 = self._contains(term, tokens)\n",
    "                positions.append((pos1, pos2))\n",
    "                tokens[pos1:pos2] = ['_' for x in range(pos1, pos2)]\n",
    "        new_tokens = []\n",
    "        for num, word in enumerate(tokens):\n",
    "            if num in [x[0] for x in positions]:\n",
    "                new_tokens.append(' <bos> ')\n",
    "            new_tokens.append(true_tokens[num])\n",
    "            if num in [x[1] - 1 for x in positions]:\n",
    "                new_tokens.append(' <eos> ')\n",
    "        return \"\".join(new_tokens)\n",
    "\n",
    "    def _contains(self, small, big):\n",
    "        for i in range(len(big) - len(small) + 1):\n",
    "            for j in range(len(small)):\n",
    "                if big[i + j] != small[j]:\n",
    "                    break\n",
    "            else:\n",
    "                return i, i + len(small)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'postag': postag,\n",
    "            'postag[:2]': postag[:2],\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]\n",
    "            postag1 = sent[i - 1][1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]\n",
    "            postag1 = sent[i + 1][1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for token, postag, label in sent]\n",
    "\n",
    "    def sent2tokens(self, sent):\n",
    "        return [token for token, postag, label in sent]\n",
    "\n",
    "    def convert_corpus(self, sents):\n",
    "        X = [self.sent2features(s) for s in sents]\n",
    "        y = [self.sent2labels(s) for s in sents]\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True)\n",
    "        crf.fit(X_train, y_train)\n",
    "        return crf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_annotate(annotations):\n",
    "    corpus = []\n",
    "    for text in annotations:\n",
    "        new = []\n",
    "        bos = False\n",
    "        ios = False\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.text == '<bos>':\n",
    "                bos = True\n",
    "            elif token.text == '<eos>':\n",
    "                ios = False\n",
    "            elif bos:\n",
    "                new.append((token.text, token.pos_, 'B'))\n",
    "                bos = False\n",
    "                ios = True\n",
    "            elif ios:\n",
    "                new.append((token.text, token.pos_, 'I'))\n",
    "            else:\n",
    "                new.append((token.text, token.pos_, 'O'))\n",
    "        corpus.append(new)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the links for 20 articles...\n",
      "Getting the texts...\n"
     ]
    }
   ],
   "source": [
    "artextr = ArticlesExtraction(20, save_txt=False, save_pdf=False)\n",
    "articles = artextr.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using only the first 1000 symbols here just to check if everything works, because it takes a lot of time\n",
    "# to process the whole text\n",
    "texts = [x[:1000] for x in list(articles.values())]\n",
    "#texts = [x for x in list(articles.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "r = RuleBasedExtractor()\n",
    "all_t = r.extract(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_terms = Counter([x.lower() for x in all_t]).most_common(1000)\n",
    "with open('popular_terms.txt', 'w') as file:\n",
    "    for line in popular_terms:\n",
    "        file.write(line[0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = Annotator()\n",
    "annotations = [anno.annotate_text(x, all_t) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bio_annotate(annotations[:15])\n",
    "test = bio_annotate(annotations[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = SequenceClassifier()\n",
    "X_train, y_train = seq.convert_corpus(train)\n",
    "model = seq.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = seq.convert_corpus(test)\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
