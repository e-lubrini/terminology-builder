{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, name = None):\n",
    "        self.name = name\n",
    "        self.points_to = dict()\n",
    "        self.stored_value = None \n",
    "        \n",
    "        self.visited = False #for searching\n",
    "\n",
    "    def point_to_node(self, other_node):\n",
    "        self.points_to[other_node.name] = other_node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.points_to == dict()\n",
    "    \n",
    "    def is_empty(self):\n",
    "        return (self.stored_value is None)\n",
    "    \n",
    "    def list_children_names(self):\n",
    "        return self.points_to.keys()\n",
    "    \n",
    "    def list_children(self):\n",
    "        self.points_to.values()\n",
    "    \n",
    "    def list_values_in_children(self):\n",
    "        stored_values = []\n",
    "        \n",
    "        if not self.is_empty():\n",
    "            stored_values.append(self.stored_value)\n",
    "\n",
    "        if self.is_leaf():\n",
    "            return stored_values\n",
    "        \n",
    "        for node in self.points_to.values():\n",
    "            stored_values.extend(\n",
    "                node.list_values_in_children()\n",
    "            )\n",
    "        return stored_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TerminologyTree():\n",
    "    def __init__(self, name = \"\", root = Node() ):\n",
    "        self.name = name\n",
    "        self.root = root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_term_to_tree(term, tree):\n",
    "\n",
    "    current_node = tree.root\n",
    "    for word in term:\n",
    "        #go to the next node\n",
    "\n",
    "            # if there is no next node, create it\n",
    "        if word not in current_node.points_to.keys():\n",
    "            new_node = Node(word)\n",
    "            current_node.points_to[word] = new_node\n",
    "        else:\n",
    "            new_node = current_node.points_to[word]\n",
    "    \n",
    "        current_node = new_node\n",
    "\n",
    "    #now we are at the end of a path whose nodes spell the term\n",
    "\n",
    "    #store the string in the end node\n",
    "    current_node.stored_value = term\n",
    "\n",
    "\n",
    "def fill_terminology_tree(term_list, tree):\n",
    "    for term in term_list:\n",
    "        add_term_to_tree(term, tree)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_term_in_tree(term, tree):\n",
    "    current_node = tree.root\n",
    "    for word in term:\n",
    "        try:\n",
    "            current_node = current_node.points_to[word]\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    if current_node.stored_value == term:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def terms_with_word(word, tree):\n",
    "    \"\"\"return a list of terms that contain the word\"\"\"\n",
    "    # breadth first search\n",
    "\n",
    "    #adapting the pseudocode from\n",
    "    #https://en.wikipedia.org/wiki/Breadth-first_search\n",
    "\n",
    "    queue = []\n",
    "    tree.root.visited = True\n",
    "    queue.append(tree.root)\n",
    "    result = []\n",
    "\n",
    "    while len(queue) != 0:\n",
    "        current_node = queue.pop()\n",
    "\n",
    "        if word == current_node.name:\n",
    "\n",
    "            # now that we found the word in a node:\n",
    "            # get all the terms stored in the children of that node\n",
    "            # and append these to the results list.\n",
    "            result.extend(\n",
    "                current_node.list_values_in_children()\n",
    "                )\n",
    "\n",
    "        for node in current_node.points_to.values():\n",
    "            if not node.visited:\n",
    "                node.visited = True\n",
    "                queue.append(node)\n",
    "    \n",
    "    _unmark_tree(tree) # visited = False for all nodes\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp = spacy.load(\"en_core_web_sm\")):\n",
    "    \n",
    "    special_case = [{ORTH: \"<bos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<bos>\", special_case)\n",
    "\n",
    "    special_case = [{ORTH: \"<eos>\"}]\n",
    "    nlp.tokenizer.add_special_case(\"<eos>\", special_case)\n",
    "\n",
    "    infixes = list([r\"'s\\b\", r\"(?<!\\d)\\.(?!\\d)\"]) +  nlp.Defaults.prefixes\n",
    "    infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "    \n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesExtraction:\n",
    "    def __init__(self, number=20, verbose=True, save_txt=True, save_pdf=True):\n",
    "        self.number = number\n",
    "        self.verbose = verbose\n",
    "        self.save_txt = save_txt\n",
    "        self.save_pdf = save_pdf\n",
    "\n",
    "    def _get_links(self):\n",
    "        if self.verbose:\n",
    "            print(f'Getting the links for {self.number} articles...')\n",
    "        mainpage = requests.get('https://as-botanicalstudies.springeropen.com/articles')\n",
    "        mainsoup = BeautifulSoup(mainpage.text)\n",
    "        links = ['https://as-botanicalstudies.springeropen.com' + x['href'] for x in\n",
    "                 sum([x.findAll('a') for x in mainsoup.findAll('h3', class_=\"c-listing__title\")], [])]\n",
    "        return links[:self.number]\n",
    "\n",
    "    def extract(self):\n",
    "        extra = ['Availability of data and materials', 'Abbreviations', 'References', 'Acknowledgements',\n",
    "                 'Funding', 'Author information', 'Ethics declarations', 'Additional information',\n",
    "                 'Rights and permissions', 'About this article']\n",
    "        links = self._get_links()\n",
    "        pdf_links = []\n",
    "        if self.verbose:\n",
    "            print('Getting the texts...')\n",
    "        texts = dict()\n",
    "        for num, link in enumerate(links):\n",
    "            if self.verbose:\n",
    "                print(f'{num + 1}/{len(links)} links', end=\"\\r\")\n",
    "            page = requests.get(link)\n",
    "            pagecontent = BeautifulSoup(page.text)\n",
    "            name = pagecontent.findAll('h1', class_=\"c-article-title\")[0].text\n",
    "            text = \"\\n\".join(sum([list(map(lambda y: y.text, x.findAll('p'))) for x in pagecontent.findAll('section') if\n",
    "                                  x.has_attr('data-title') and x['data-title'] not in extra], []))\n",
    "            texts[name] = text\n",
    "            pdf_link = [x.findAll('a') for x in pagecontent.findAll('div', class_=\"c-pdf-download u-clear-both\")][0][0][\n",
    "                'href']\n",
    "            pdf_links.append(pdf_link)\n",
    "        if self.save_txt:\n",
    "            if self.verbose:\n",
    "                print('Saving the articles in txt...')\n",
    "            if not os.path.exists('articles'):\n",
    "                os.mkdir('articles')\n",
    "            for key, value in texts.items():\n",
    "                with open(f\"articles/{key.replace('/', '|')}.txt\", 'w') as file:\n",
    "                    file.write(value)\n",
    "        if self.save_pdf:\n",
    "            if self.verbose:\n",
    "                print('Saving the articles in pdf...')\n",
    "            if not os.path.exists('articles_pdf'):\n",
    "                os.mkdir('articles_pdf')\n",
    "            for (key, value), link in zip(texts.items(), pdf_links):\n",
    "                pdf = requests.get('https:' + link, allow_redirects=True)\n",
    "                open(f\"articles_pdf/{key.replace('/', '|')}.pdf\", 'wb').write(pdf.content)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "\n",
    "    def extract(self, texts):\n",
    "        all_terms = []\n",
    "        self._add_rules()\n",
    "        for num, text in enumerate(texts):\n",
    "            doc = self.nlp(text)\n",
    "            matches = self.matcher(doc)\n",
    "            for match_id, start, end in matches:\n",
    "                string_id = self.nlp.vocab.strings[match_id]\n",
    "                span = doc[start:end]\n",
    "                lemma = ' '.join([n.lemma_ for n in self.nlp(span.text.lower())])\n",
    "                all_terms.append(lemma)\n",
    "\n",
    "            print(f'{num + 1}/{len(texts)} texts processed', end=\"\\r\")\n",
    "        return all_terms\n",
    "\n",
    "    def _add_rules(self):\n",
    "        noun_pattern = {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        det_pattern = {\"POS\": {\"IN\": [\"DET\", \"PRON\"]}, \"OP\": \"?\"}\n",
    "        pattern = [  #[{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, noun_pattern],\n",
    "            #[{\"DEP\": \"compound\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, noun_pattern],\n",
    "            [{\"POS\": \"ADJ\", \"OP\": \"+\"}, noun_pattern],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, det_pattern, noun_pattern],\n",
    "            [det_pattern, {\"POS\": \"ADJ\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"ADJ\"}, noun_pattern],\n",
    "        ]\n",
    "        self.matcher.add(\"terms\", pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator:\n",
    "    tagging = dict()\n",
    "    def __init__(self):        \n",
    "        self.nlp = custom_tokenizer(nlp)\n",
    "\n",
    "    def _longest_term(position, word_list, tree):\n",
    "        current_node  = tree.root\n",
    "        term = []\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                word = word_list[position]\n",
    "                current_node = current_node.points_to[word]\n",
    "                if not current_node.is_empty(): \n",
    "                    aux_term = current_node.stored_value\n",
    "                    if len(aux_term) > len(term): term = aux_term\n",
    "                position += 1\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        return term\n",
    "\n",
    "\n",
    "\n",
    "    def annotate(self, tree, word_list):\n",
    "        \"\"\"\"\n",
    "        returns a list of tuples (word, tag)\n",
    "        \"\"\"\n",
    "        tagging = dict()\n",
    "        position = 0\n",
    "\n",
    "        def put_tag(position, tag):\n",
    "            tagging[str(position)]= tag\n",
    "            return None\n",
    "\n",
    "        while position < len(word_list):\n",
    "            #find the longest term appearing in the text at this position\n",
    "            term = self._longest_term(position, word_list, tree)\n",
    "            \n",
    "            length = len(term)\n",
    "\n",
    "            if length == 0: # no term was found\n",
    "                put_tag(position, \"O\")\n",
    "                position += 1\n",
    "            else:\n",
    "                put_tag(position, \"B\") #beginning of term\n",
    "                for i in range(length-1):\n",
    "                    put_tag(position + i +1, \"I\") #inside of term\n",
    "                position += length\n",
    "\n",
    "        text_tags = [\n",
    "            (word_list[position], tagging[str(position)])\n",
    "            for position in range(len(word_list))\n",
    "        ]\n",
    "\n",
    "        return text_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def word2features(self, sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'postag': postag,\n",
    "            'postag[:2]': postag[:2],\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]\n",
    "            postag1 = sent[i - 1][1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]\n",
    "            postag1 = sent[i + 1][1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "\n",
    "        return features\n",
    "\n",
    "    def sent2features(self, sent):\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    def sent2labels(self, sent):\n",
    "        return [label for token, postag, label in sent]\n",
    "\n",
    "    def sent2tokens(self, sent):\n",
    "        return [token for token, postag, label in sent]\n",
    "\n",
    "    def convert_corpus(self, sents):\n",
    "        X = [self.sent2features(s) for s in sents]\n",
    "        y = [self.sent2labels(s) for s in sents]\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True)\n",
    "        crf.fit(X_train, y_train)\n",
    "        return crf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the links for 20 articles...\n",
      "Getting the texts...\n"
     ]
    }
   ],
   "source": [
    "artextr = ArticlesExtraction(20, save_txt=False, save_pdf=False)\n",
    "articles = artextr.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "r = RuleBasedExtractor()\n",
    "all_t = r.extract(texts)\n",
    "terminology = [t.split(' ') for t in all_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_terms = Counter([x.lower() for x in all_t]).most_common(1000)\n",
    "with open('popular_terms.txt', 'w') as file:\n",
    "    for line in popular_terms:\n",
    "        file.write(line[0] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tree Representation for Terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'salt stress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27376/3650559603.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTerminologyTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfill_terminology_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints_to\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'salt stress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'salt stress'"
     ]
    }
   ],
   "source": [
    "tree = TerminologyTree()\n",
    "fill_terminology_tree(terminology, tree)\n",
    "tree.root.points_to['salt stress']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_longest_term' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27376/1418890822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnnotator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_27376/1418890822.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnnotator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_27376/3048327921.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, tree, word_list)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#find the longest term appearing in the text at this position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_longest_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_longest_term' is not defined"
     ]
    }
   ],
   "source": [
    "anno = Annotator()\n",
    "annotations = [anno.annotate(x, all_t) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bio_annotate(annotations[:15])\n",
    "test = bio_annotate(annotations[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = SequenceClassifier()\n",
    "X_train, y_train = seq.convert_corpus(train)\n",
    "model = seq.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = seq.convert_corpus(test)\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
