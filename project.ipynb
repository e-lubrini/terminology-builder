{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesExtraction:\n",
    "    def __init__(self, number=20, verbose=True):\n",
    "        self.number = number\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _get_links(self):\n",
    "        if self.verbose:\n",
    "            print(f'Getting the links for {self.number} articles...')\n",
    "        mainpage = requests.get('https://as-botanicalstudies.springeropen.com/articles')\n",
    "        mainsoup = BeautifulSoup(mainpage.text)\n",
    "        links = ['https://as-botanicalstudies.springeropen.com' + x['href'] for x in\n",
    "                 sum([x.findAll('a') for x in soup.findAll('h3', class_=\"c-listing__title\")], [])]\n",
    "        return links[:self.number]\n",
    "\n",
    "    def extract(self):\n",
    "        extra = ['Availability of data and materials', 'Abbreviations', 'References', 'Acknowledgements',\n",
    "                 'Funding', 'Author information', 'Ethics declarations', 'Additional information',\n",
    "                 'Rights and permissions', 'About this article']\n",
    "        links = self._get_links()\n",
    "        if self.verbose:\n",
    "            print('Getting the texts...')\n",
    "        texts = dict()\n",
    "        for num, link in enumerate(links):\n",
    "            if self.verbose:\n",
    "                print(f'{num + 1}/{len(links)} links', end=\"\\r\")\n",
    "            page = requests.get(link)\n",
    "            pagecontent = BeautifulSoup(page.text)\n",
    "            name = pagecontent.findAll('h1', class_=\"c-article-title\")[0].text\n",
    "            #print(pagecontent.findAll('section'))\n",
    "            text = \"\\n\".join(sum([list(map(lambda y: y.text, x.findAll('p'))) for x in pagecontent.findAll('section') if\n",
    "                                  x.has_attr('data-title') and x['data-title'] not in extra], []))\n",
    "            texts[name] = text\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def extract_and_save(self):\n",
    "        texts = self.extract()\n",
    "        if self.verbose:\n",
    "            print('Saving the articles...')\n",
    "        if not os.path.exists('articles'):\n",
    "            os.mkdir('articles')\n",
    "        for key, value in texts.items():\n",
    "            with open(f\"articles/{key.replace('/', '|')}\", 'w') as file:\n",
    "                file.write(value)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "\n",
    "    def extract(self, texts):\n",
    "        all_terms = []\n",
    "        self._add_rules()\n",
    "        for num, text in enumerate(texts):\n",
    "            doc = self.nlp(text)\n",
    "            matches = matcher(doc)\n",
    "            for match_id, start, end in matches:\n",
    "                string_id = self.nlp.vocab.strings[match_id]\n",
    "                span = doc[start:end]\n",
    "                lemma = ' '.join([n.lemma_ for n in self.nlp(span.text.lower())])\n",
    "                all_terms.append(lemma)\n",
    "\n",
    "            print(f'{num + 1}/{len(texts)} texts processed', end=\"\\r\")\n",
    "\n",
    "    def _add_rules(self):\n",
    "        noun_pattern = {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        det_pattern = {\"POS\": {\"IN\": [\"DET\", \"PRON\"]}, \"OP\": \"?\"}\n",
    "        pattern = [  #[{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, noun_pattern],\n",
    "            #[{\"DEP\": \"compound\"}, {\"POS\": \"NOUN\"}],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, noun_pattern],\n",
    "            [{\"POS\": \"ADJ\", \"OP\": \"+\"}, noun_pattern],\n",
    "            [noun_pattern, {\"POS\": \"ADP\"}, det_pattern, noun_pattern],\n",
    "            [det_pattern, {\"POS\": \"ADJ\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"ADJ\"}, noun_pattern],\n",
    "        ]\n",
    "        self.matcher.add(\"terms\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def annotate_text(text):\n",
    "        doc = nlp(text)\n",
    "        true_tokens = [x.text_with_ws for x in doc]\n",
    "        positions = []\n",
    "        tokens = [x.lemma_.lower() for x in doc]\n",
    "        for term in all_terms:\n",
    "            term = term.split()\n",
    "            while contains(term, tokens):\n",
    "                pos1, pos2 = contains(term, tokens)\n",
    "                positions.append((pos1, pos2))\n",
    "                tokens[pos1:pos2] = ['_' for x in range(pos1, pos2)]\n",
    "        new_tokens = []\n",
    "        for num, word in enumerate(tokens):\n",
    "            if num in [x[0] for x in positions]:\n",
    "                new_tokens.append(' <bos> ')\n",
    "            new_tokens.append(true_tokens[num])\n",
    "            if num in [x[1] - 1 for x in positions]:\n",
    "                new_tokens.append(' <eos> ')\n",
    "        return \"\".join(new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}